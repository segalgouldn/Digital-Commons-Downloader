{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pathlib\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_from_soup(soup):\n",
    "    non_pdf_metadata_list = soup.find_all(\"p\", \"metadata-only\")\n",
    "    non_pdf_metadata_article_list = [non_pdf_metadata.next_sibling.next_sibling.next_sibling \n",
    "                                     for non_pdf_metadata in non_pdf_metadata_list]\n",
    "    \n",
    "    for non_pdf_metadata in non_pdf_metadata_list:\n",
    "        non_pdf_metadata.decompose()\n",
    "    for non_pdf_metadata_article in non_pdf_metadata_article_list:\n",
    "        non_pdf_metadata_article.decompose()\n",
    "        \n",
    "    non_pdf_link_list = soup.find_all(\"p\", \"external\")\n",
    "    non_pdf_link_article_list = [non_pdf_link.next_sibling.next_sibling.next_sibling \n",
    "                                 for non_pdf_link in non_pdf_link_list]\n",
    "    \n",
    "    for non_pdf_link in non_pdf_link_list:\n",
    "        non_pdf_link.decompose()\n",
    "    for non_pdf_link_article in non_pdf_link_article_list:\n",
    "        non_pdf_link_article.decompose()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_senior_project_information(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    clean_soup = remove_excess_from_soup(soup)\n",
    "    \n",
    "    pagination = clean_soup.find(\"div\", {\"class\": \"adjacent-pagination\"})\n",
    "    if pagination:\n",
    "        all_urls = [a.get(\"href\") for a in pagination.find_all(\"a\")]\n",
    "        all_soups = [remove_excess_from_soup(BeautifulSoup(requests.get(url).content, \"html.parser\")) \n",
    "                     for url in all_urls]\n",
    "    else: \n",
    "        all_soups = [clean_soup]    \n",
    "    \n",
    "    pdf_links_in_soups = [soup.find_all(\"p\", \"pdf\") for soup in all_soups]\n",
    "    article_links_in_soups = [soup.find_all(\"p\", \"article-listing\") for soup in all_soups]\n",
    "    \n",
    "    grouped_links_in_soups = [{\"pdf\": pdf, \"article\": article} \n",
    "                              for pdf_links, article_links in zip(pdf_links_in_soups, article_links_in_soups) \n",
    "                              for pdf, article in zip(pdf_links, article_links)]\n",
    "    \n",
    "    return [{\"title\": grouped_links[\"article\"].a.getText(),\n",
    "             \"author\": grouped_links[\"article\"].a.next_sibling[2:],\n",
    "             \"url\": grouped_links[\"pdf\"].a.get(\"href\")} \n",
    "            for grouped_links in grouped_links_in_soups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_collections(url=\"http://digitalcommons.bard.edu/undergraduate/\"):\n",
    "    links_main = BeautifulSoup(requests.get(url).content, \"html.parser\").find(\"div\", {\"id\":\"series-home\"}).find_all(\"a\", {\"class\": False})\n",
    "    links_final = [BeautifulSoup(requests.get(link.get(\"href\")).content, \"html.parser\").find(\"div\", {\"id\":\"series-home\"}).find(\"a\", {\"class\": False}) for link in links_main if link is not None]\n",
    "    links_final = filter(None, links_final)\n",
    "    return [{\"name\": link.get(\"title\"), \n",
    "             \"url\": link.get(\"href\"), \n",
    "             \"data\": grab_senior_project_information(link.get(\"href\"))} for link in links_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(data):\n",
    "    for folder in data:\n",
    "        pathlib.Path(\"./\" + folder[\"name\"]).mkdir()\n",
    "        for sproj in folder[\"data\"]:\n",
    "            try:\n",
    "                with open((\"./\" \n",
    "                           + folder[\"name\"] \n",
    "                           + \"/\" \n",
    "                           + sproj[\"title\"].replace(\":\", \"- \").replace(\"/\", \"-\") \n",
    "                           + \" by \" \n",
    "                           + sproj[\"author\"] \n",
    "                           + \".pdf\"), \"wb\") as f:\n",
    "                    f.write(requests.get(sproj[\"url\"]).content)\n",
    "            except Exception as e:\n",
    "                print(\"{}: Unable to download the following file:\\n{}\".format(e, sproj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "collections = grab_collections()\n",
    "with open(\"all_senior_projects.json\", \"w\") as injson:\n",
    "    json.dump(collections, injson, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "download_pdfs(collections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
